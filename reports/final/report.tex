\documentclass[sigconf,nonacm]{acmart}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

\title{CS145 Team 22 Midterm Report}

\author{Hamlin Liu}
\affiliation{%
  \institution{UCLA, 805103522}
  }
\email{hamlin.liu@gmail.com}
\author{Shriniket Buche}
\affiliation{%
  \institution{UCLA, 305088562}
  }
\email{shriniketbuche@gmail.com}
\author{Juan Estrada}
\affiliation{%
  \institution{UCLA, 105347991}
  }
\email{juanestrada@ucla.edu}
\author{Yash Lala}
\affiliation{%
  \institution{UCLA, 905159212}
  }
\email{yashlala@gmail.com}
\author{Justin Yi}
\affiliation{%
  \institution{UCLA, 905123893}
  }
\email{joostinyi00@gmail.com}
\renewcommand{\shortauthors}{Team 22}

\begin{abstract}

COVID-19, the theme of our CS 145 project, has been classified as a worldwide
pandemic. The rate of its spread is a complex function of local social
distancing, government policy, climate, and temporal factors, just to name a
few. Detailed analysis of all of these contributory causes is not humanly
possible. However, we can build reasonably accurate disease models by using the
Data Mining techniques taught in CS 145. This report documents our team’s
efforts towards analyzing COVID-related data in over the majority of the year
to allow for predictive forecasting. 

We catalogue our problem at hand, initial data preprocessing, exploratory
efforts toward modeling our problem, and our future predictions as we construct
a model capable of predicting fatalities caused by COVID-19, namely through the
use of a hybrid linear, exponential, and autoregressive models. 

\end{abstract}

\maketitle

\section{Introduction}

\subsection{Background}

The increasing spread of COVID-19 poses a substantial impact on the status of
global health and economy, and having accurate forecasts of the number of
affected can better inform local and national governmental bodies to make
decisions to promote public health and safety and keeping the economy afloat.
As we position to tackle this problem, we inspect our the data we have on hand:
we were given reports of various state-wide features of the COVID-19 infection
profile from as early as April, we are specifically interested in the ability
to predict the number of cases and deaths for a particular state.

\subsection{Related Work}
Notable advances in this domain are noted. Nesteruk \cite{Nesteruk} models the
progression of the pandemic using the Susceptible, Infected, Removed (SIR)
paradigm, which aims to find correlated differential equations to model flow
between different partitions of the studied population. 
\cite{EJMO} uses more direct curve estimation methods such as Holt and ARIMA on
the number of coronavirus cases in of interest, (e.g. Germany, Japan, Turkey,
the UK, etc.). These models predict the actual number of coronavirus cases to a
high degree of statistical significance. 
Others have applied novel iterative methods based on cubic spline interpolation
and Euler's methods towards predicting the spread of the virus \cite{APPADU2020}.

\section{Data Preprocessing}
Now moving on to actually wrangling the data for processing and straightforward
input to our prospective models, we first needed to examine the existing data
to analyze where help was needed. 

Upon inspection of data types, it is noted
that much of our data is numerical, bar the categorical and ordinal columns of
Province\_State and Date, respectively. We chose to convert the date from a
string type to datetime object type, to ease future parsing. Using our
contextual knowledge of our problem space, we chose to address the modeling
problem from an individual Province\_State perspective, i.e. we grouped our
data by the Province\_State field. From here on, we will refer to this
“Province\_State” field simply as the “state”. As a group we went back and
forth on whether having so many models would result in higher variance due to
the increased complexity of the model, but we decided on separate models due to
different restrictions imposed by policy makers on interstate travel and the
general thought that most people would be sheltering in place, and thus would
not travel between states.

\subsection{Round 1}
Our initial imputation strategy went as follows: We examined each state’s
numeric data column by column. If null values comprised more than 70\% of the
total data in the column, we dropped the entire column; otherwise, we replaced
all null values with the median value in our column. After noting that this
approach led to “plateaus” in our data, we tried linearly interpolating the
values in each column. This approach produces more organic data in each column,
but fails to impute data at the beginning of a time series (due to having no
“base point” to interpolate off of). Because our current models treat every
column separately, we don’t anticipate that this will cause issues; we can drop
NaN values to create new “columns” that are perfectly interpolated, but start
at a later date.  

After interpolating the data, we normalized it to zero mean unit variance. At
this time, we are still considering whether to normalize the data globally or
within a single column. Intuition would suggest that a column-wise approach
would be best. We performed these steps before really definitively knowing what
sort of modeling approach we were interested in taking, as we will discuss,
this normalization was ultimately unused, since we were working with more
direct regression methods that did not benefit from this scaling, but did drop
the null data points.

\subsection{Round 2}
At this point in time, we decided on scrapping our explorations of different
forecasting implementations to be detailed below, and focused on univariate
feature fitting methodologies for forecasting a function to fit the observed
count of confirmed cases and deaths, namely that of Holt-Winters exponential
smoothing, ARIMA for forecasting, and a simple linear model from a particular
day offset. As a result, we did not normalize our data and just allowed the
models to drop the null values from the training dataset.

However, for this round, we needed to obtain new data since our round 2 dataset
only went up to November 22nd. To do this, we created a simple Python web scraper that 
would download the new daily files from the Johns Hopkins University CSSE COVID-19 
dataset \cite{JHUdataset} which updates everyday with a new comma-separated-values file.
Thus we were able to obtain the confirmed COVID-19 case and death count of each state
within the US up to December 5th, 2020.


\section{Problem Definition}
We are given a dataset of various COVID-related features (eg. testing rate,
deaths, confirmed cases), as well as a dataset of various movement-related data
(traffic flow into and out of two states, respectively). 
We must train a machine learning model on this data, and use it to predict the
future rates of death and confirmed cases of COVID-19. 

The competition is divided into 2 rounds. The first round was to predict the
case and death count of each of the 50 states from September 1st to September
26th given any COVID-19 related data from April 12th to August 31st. The second
round was to predict the case and death count of each of the 50 states during
the winter holiday surge, specifically from December 7th to December 12th. 
For
the second round, the method can train on any any COVID-19 related data from
April 12th to December 6th. 

Our model's accuracy will be gauged via the Mean Absolute Percentage Error
(MAPE) metric, which is defined as the average of the absolute error over all
data points as per the equation below. 

$$MAPE = \frac{1}{n} \sum_{i = 1}^{n} |\frac{p_i - a_i}{a_i} |$$

$n$ is defined as the size of the dataset. $p_i$ symbolizes the predicted value
of a feature at index $i$. $a_i$ symbolizes the actual value of the feature at
index $i$.


\section{Model Performance}
\subsection{Linear Regression}
\subsection{ARIMA}
\subsection{Exponential Smoothing}
\subsection{Long Short Term Memory Neural Networks}

Neural networks are shown to be incredibly malleable since there is a plentiful
amount of hyperparameters to tune and to create ranging from the type of
activation function to the architecture of the network. For sequential data
specifically, recurrent neural networks or RNNs work well mostly because it
allows previous outputs to be used as inputs while having hidden states.  
In the context of time series prediction and the task at hand, RNNs work well
since it can utilize previous timestamped or predicted data to predict other
output data. Inside each layer and node are specific functions that manipulate
the inputs known as gates.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/architecture-rnn-ltr.png}
  \caption{Example of RNN: $x$ is the input, $y$ is the output, the middle blue
  cells are a part of the hidden layer}
  \label{fig:rnn}
\end{figure}

Within the context of RNNs, there is a set of specific RNNs called Long Short
Term Memory Neural Networks or LSTM for short. What is specific of the LSTM is
that it utilizes a specific set of 3 gates: an update, output, relvance, and
forget gate \cite{LSTMlecture}. This allows LSTM neural networks to regulate
the flow of information and decide how much past data affects the current
prediction, generalizing better for time-series specific tasks.

Given this context, and before using this method as our main modeling choice,
we needed to validate this method. Using the PyTorch neural network framework
\cite{Pytorch}, a simple RNN neural network of 1 layer with 50 LSTM nodes was
tested on a subset of the round 1 data. 
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/LSTMPytorch.png}
  \caption{LSTM predictions on data pertaining to California}
  \label{fig:LSTM_trial}
\end{figure} 

The experimentation procedure would use the confirmed case and death count data
from April 12th to August 17th for the state of California and then validating
the MAPE over a period of 14 days, August 18th to August 31st. The neural
network will be trained over a varied amount of epochs and learning rates. 
Attached on Figure  \ref{fig:LSTM_trial} is an example trial run of our
prediction using the PyTorch neural network. The MAPE on this was around $6.93$
which was much higher than the other methods.

Given the lackluster results, amount of time required to tune each parameter,
and the non-deterministic nature of neural networks, the use of LSTM neural
networks was not really explored deeply with the rest of the dataset.

\subsection{Conclusions}

While the HOLT model typically gives us the lowest overall MAPE, it
consistently underperforms on certain states. Linear Regression and ARIMA may
yield significantly better results for these "problem states". However, it is
not always easy to tell which states are going to cause problems with the HOLT
model -- performance varies significantly based on the shape of the latest
batch of test data. This heterogeneity makes it very difficult to pick a
"favored model" for our time series predictions. 

Methods such as LSTMs and Prophet were too complex for our use case and
time duration. Given the amount of time we had and the necessary work needed 
, we could not optimize these methods and their hyper parameter for our
situation and thus we disregarded using these methods for the final code
submission.

Not all of our results were mixed. While model performance varied significantly
based on the target state, the "optimal hyperparameters" used to train these
models do not change significantly on a state-by-state basis. By performing
grid search on each model's hyperparameters, we developed a group of
parameterized models that handle general state data reasonably well. 


\section{Final Model Design}

The pros and cons (as well as the relative efficacy) of each model have already
been discussed. Faced with such a diversity of model performance, we opted to
use a hybrid model for our overall submission. 

Both rounds used a univariate data model (ie. trained only on the number of
deaths or confirmed cases), and were trained state-by-state (ie. a new model
was trained on each state's data; this model's predictions became our
predictions for the given state). An overview of our methodology follows. 

\subsection{Round 1}

Our Round 1 modelling method used a comparatively straightfoward "hybrid"
approach. We split our training data by state, then manually ran HOLT and
linear regression models on the state's data (using the last few weeks as
validation data). We noted which models gave us a lower MAPE for which states,
and hardcoded these $(state, model)$ associations into our script, assuming
that these trends would hold. For example, linear regression usually
outperformed HOLT for predicting deaths in the state of Montana; we accordingly
instructed the computer to always use linear regression when predicting
Montana's deaths. 

However, this mapping wasn't sufficient. 
Model performance is a function of the total amount of data given to the model;
for time series data, this includes our start date. Linear regression, for
example, may perform better when given only the last 2 weeks of data (to
understand why, consider the effect of step size on linear gradient descent
when trying to fit an exponential curve). 
Every model was thus given a \emph{start date} parameter, which was defined as
the date of the oldest training datapoint that the model received. 
For Round 1, we set this parameter manually on a state-by-state basis by
hardcoding it into our script. 

After building up this database of optimal start dates and optimal models, we
wrote a Python script that trained the given models (using \cite{scikit-learn}'s
\texttt{linear\_model.LinearRegression} class and the \texttt{Holt} module from
the \cite{statsmodels} library). We then used the library routines to give us
time series predictions for the required number of days. 

\subsection{Round 2}

After completing Round 1 of the project, we began to realize that manually
cherry-picking an appropriate model for each state was not scalable. 

We had no way to test quick adjustments to our hyperparameters, which made
manual optimization tedious. In addition, we had no way of effectively
determining the best \emph{start\_date} for a given model on a given state. 
We opted to shift towards an automated approach towards model selection. 
Otherwise, our methodology remained the same as in Round 1, where we would test
multiple \emph{start\_dates} and validate them over a small validation set segregated
from the original training data.

Suppose that we are given $n$ days of training data, and are tasked with
predicting $m$ days in the future. 

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/Final_model.jpg}
  \caption{An flowchart of what our Round 2 model does}
  \label{fig:model_final}
\end{figure}

First, we segregate the data again. Given $n$ days of training data, our model
uses the first $n-2$ days to train on (we'll call this set $T$) and the last
$2$ days as a validation dataset (we'll call this subdivision $V$). We then
train Linear Regression, HOLT, and ARIMA models on $T$, predicting $m+2$ days
into the future. We take the first $2$ days of these predictions, and calculate
the MAPE using $V$. After comparing the MAPEs of each model on $V$, we choose
the model that gives us the lowest MAPE for that particular state, and return
the remaining $m$ predicted days. An example iteration can be seen in the 
Figure \ref{fig:model_final} which shows a visual flowchart of what is being done
for each state.

After observing our hybrid model, we noted two trends: 
\begin{itemize}
\item 
Splitting our data into $T$ and $V$ caused some of our models to
underperform. This is because they effectively had to predict $m+2$ days into
the future as opposed to $m$ days (the first two days, of course, were used for
our validation and model selection). This underperformance wasn't uniform;
HOLT tended to give us relatively accurate predictions, while linear regression
tended to underpredict the number of deaths. 
\item
Different models often give similar MAPEs when evaluated on $V$. 
\end{itemize}

Given these two points, we added an $\epsilon$ "margin" term to our final
implementation. If two models perform comparatively (ie. within $\epsilon$ MAPE
of each other) on $V$, then our framework prefers the model that "generalizes
better" as per our heuristic observations. For example, if linear regression
and HOLT both get an MAPE of $8.675309 \pm (\epsilon = 0.001)$, our framework
will use HOLT to predict the values for the state. Our framework prefers HOLT
to linear regression if the MAPEs are similar, but doesn't put ARIMA into this
"margin" framework (ie. if ARIMA yields the lowest MAPE, ARIMA will always be
chosen). The addition of this $\epsilon$ parameter yielded an experimental 
MAPE improvement of around 0.05. 

\section{Conclusion}

\section{Task Distribution Form}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         Task & People  \\
         \hline \hline
         Exploratory Data Analysis & Hamlin, Juan, Justin, Shrin, Yash\\
         \hline
         Normalization code & Justin\\
         \hline
         Linear Regression testing & Juan\\
         \hline
         ARIMA testing & Shrin\\
         \hline
         AR testing & Shrin\\
         \hline
         HOLT testing & Juan, Shrin\\
         \hline
         LSTM and BiLSTM model testing & Hamlin, Shrin \\
         \hline
         VAR model testing & Justin, Shrin\\
         \hline
         Gluonts, Prophet, and BATS model testing & Shrin\\
         \hline
         Data Clustering & Justin, Yash\\
         \hline
         Data Interpolation & Yash\\
         \hline
         Midterm Report Writing & Hamlin, Juan, Justin, Shrin, Yash\\
         \hline
         Midterm Report Formatting & Hamlin, Yash\\
         \hline
         Scraping for new data & Hamlin\\
         \hline
         Kaggle Submissions & Juan, Shrin\\
         \hline
         Round 1 code & Hamlin, Juan\\
         \hline
         Round 2 code & Hamlin, Justin, Yash\\
         \hline
         Final Report writing & Hamlin, Juan, Justin, Shrin, Yash\\
         \hline
    \end{tabular}
\end{table}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}
\endinput
