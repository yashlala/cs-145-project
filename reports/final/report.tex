\documentclass[sigconf,nonacm]{acmart}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

\title{CS145 Team 22 Midterm Report}

\author{Hamlin Liu}
\affiliation{%
  \institution{UCLA, 805103522}
  }
\email{hamlin.liu@gmail.com}
\author{Shriniket Buche}
\affiliation{%
  \institution{UCLA, 305088562}
  }
\email{shriniketbuche@gmail.com}
\author{Juan Estrada}
\affiliation{%
  \institution{UCLA, 105347991}
  }
\email{juanestrada@ucla.edu}
\author{Yash Lala}
\affiliation{%
  \institution{UCLA, 905159212}
  }
\email{yashlala@gmail.com}
\author{Justin Yi}
\affiliation{%
  \institution{UCLA, 905123893}
  }
\email{joostinyi00@gmail.com}
\renewcommand{\shortauthors}{Team 22}

\begin{abstract}

COVID-19, the theme of our CS 145 project, has been classified as a worldwide pandemic. 
The rate of its spread is a complex function of local social distancing, government policy, 
climate, and temporal factors, just to name a few. Detailed analysis of all of these contributory 
causes is not humanly possible. However, we can build reasonably accurate disease models by using the 
Data Mining techniques taught in CS 145. This report documents our team’s efforts towards analyzing 
COVID-related data in over the majority of the year to allow for predictive forecasting. 

We catalogue our problem at hand, initial data preprocessing, exploratory efforts toward modeling our problem, 
and our future predictions as we construct a model capable of predicting fatalities caused by COVID-19, 
namely through the use of a hybrid linear, exponential, and autoregressive models. 

\end{abstract}

\maketitle

\section{Introduction of overall goal and background}
The increasing spread of COVID-19 poses a substantial impact on the status of global health and economy, 
and having accurate forecasts of the number of affected can better inform local and national governmental 
bodies to make decisions to promote public health and safety and keeping the economy afloat. 
As we position to tackle this problem, we inspect our the data we have on hand: we were given reports of various 
state-wide features of the COVID-19 infection profile from as early as April, we are specifically interested in the ability 
to predict the number of cases and deaths for a particular state.

\section{Related Work}
Notable advances in this domain are noted, Nesteruk \cite{Nesteruk} employs the Susceptible, Infected, Removed (SIR) paradigm
– which aims to find different correlated differential equations to model flow between different partitions of the 
studied population – and the subsequent progression of the pandemic. Yonar \cite{EJMO} uses different more direct curve estimation methods
on the number of coronavirus cases, such as Holt and ARIMA, for different affected countries of interest, (e.g. Germany, Japan, Turkey, UK, .etc),
which are shown to be statistically significant. Different iterative methods, based on cubic spline interpolation and Euler's methods
are more novel developments in pursuit of accurately representing the spread of the virus \cite{APPADU2020}.

\section{Model Performance}
\subsection{Linear Regression}
\subsection{ARIMA}
\subsection{HOLT}
\subsection{Conclusions}

While the HOLT model typically gives us the lowest overall MAPE, it
consistently underperforms on certain states. Linear Regression and ARIMA may
yield significantly better results for these "problem states". However, it is
not always easy to tell which states are going to cause problems with the HOLT
model -- performance varies significantly based on the shape of the latest
batch of test data. This heterogeneity makes it very difficult to pick a
"favored model" for our time series predictions. 

Not all of our results were mixed. While model performance varied significantly
based on the target state, the "optimal hyperparameters" used to train these
models do not change significantly on a state-by-state basis. By performing
grid search on each model's hyperparameters, we developed a group of
parameterized models that handle general state data reasonably well. 


\section{Model Design}

The pros and cons (as well as the relative efficacy) of each model have already
been discussed. Faced with such a diversity of model performance, we opted to
use a hybrid model for our overall submission. Rather than cherry picking the
best-performing models for each state, we opted to use an automated approach. 

Suppose we have to predict $m$ days into the future. 

Our model first segregates the data by state. For each state,

\section{TODO TODO TODO TODO}
% did I mention this is TODO?

First, we segregate the data. Given $n$ days of training data, our model uses
the first $n-2$ days to train on (we'll call this set $T$) and the last $2$
days as a validation dataset (we'll call this subdivision $V$). 
We then train Linear Regression, HOLT, and ARIMA models on $T$, predicting
$m+2$ days into the future. We take the first $2$ days of these predictions,
and calculate the MAPE using $V$. After comparing the MAPEs of each model on
$V$, we choose the model that gives us the lowest MAPE for that particular
state, and return the remaining $m$ days of predictions. 

After observing our hybrid model, we noted two trends: 
\begin{itemize}
\item 
Splitting our data into $T$ and $V$ caused some of our models to
underperform. This is because they effectively had to predict $m+2$ days into
the future as opposed to $m$ days (the first two days, of course, were used for
our validation and model selection). This underperformance wasn't uniform;
HOLT tended to give us relatively accurate predictions, while linear regression
tended to underpredict the number of deaths. 
\item
Different models often give similar MAPEs when evaluated on $V$. 
\end{itemize}

Given these two points, we added an $\epsilon$ "margin" term to our final
implementation. If two models perform comparatively (ie. within $\epsilon$ MAPE
of each other) on $V$, then our framework prefers the model that "generalizes
better" as per our heuristic observations. For example, if linear regression
and HOLT both get an MAPE of $8.675309 \pm (\epsilon = 0.001)$, our framework
will use HOLT to predict the values for the state. Our framework prefers HOLT
to linear regression if the MAPEs are similar, but doesn't put ARIMA into this
"margin" framework (ie. if ARIMA yields the lowest MAPE, ARIMA will always be
chosen). 

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}
\endinput
