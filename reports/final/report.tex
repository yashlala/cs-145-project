\documentclass[sigconf,nonacm]{acmart}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

\title{CS145 Team 22 Midterm Report}

\author{Hamlin Liu}
\affiliation{%
  \institution{UCLA, 805103522}
  }
\email{hamlin.liu@gmail.com}
\author{Shriniket Buche}
\affiliation{%
  \institution{UCLA, 305088562}
  }
\email{shriniketbuche@gmail.com}
\author{Juan Estrada}
\affiliation{%
  \institution{UCLA, 105347991}
  }
\email{juanestrada@ucla.edu}
\author{Yash Lala}
\affiliation{%
  \institution{UCLA, 905159212}
  }
\email{yashlala@gmail.com}
\author{Justin Yi}
\affiliation{%
  \institution{UCLA, 905123893}
  }
\email{joostinyi00@gmail.com}
\renewcommand{\shortauthors}{Team 22}

\begin{abstract}

COVID-19, the theme of our CS 145 project, has been classified as a worldwide
pandemic. The rate of its spread is a complex function of local social
distancing, government policy, climate, and temporal factors, just to name a
few. Detailed analysis of all of these contributory causes is not humanly
possible. However, we can build reasonably accurate disease models by using the
Data Mining techniques taught in CS 145. This report documents our team’s
efforts towards analyzing COVID-related data in over the majority of the year
to allow for predictive forecasting. 

We catalogue our problem at hand, initial data preprocessing, exploratory
efforts toward modeling our problem, and our future predictions as we construct
a model capable of predicting fatalities caused by COVID-19, namely through the
use of a hybrid linear, exponential, and autoregressive models. 

\end{abstract}

\maketitle

\section{Introduction}

\subsection{Background}

The increasing spread of COVID-19 poses a substantial impact on the status of
global health and economy, and having accurate forecasts of the number of
affected can better inform local and national governmental bodies to make
decisions to promote public health and safety and keeping the economy afloat.
As we position to tackle this problem, we inspect our the data we have on hand:
we were given reports of various state-wide features of the COVID-19 infection
profile from as early as April, we are specifically interested in the ability
to predict the number of cases and deaths for a particular state.

\subsection{Related Work}

Notable advances in this domain are noted, Nesteruk \cite{Nesteruk} employs the
Susceptible, Infected, Removed (SIR) paradigm – which aims to find different
correlated differential equations to model flow between different partitions of
the studied population – and the subsequent progression of the pandemic. Yonar
\cite{EJMO} uses different more direct curve estimation methods on the number
of coronavirus cases, such as Holt and ARIMA, for different affected countries
of interest, (e.g. Germany, Japan, Turkey, UK, .etc), which are shown to be
statistically significant. Different iterative methods, based on cubic spline
interpolation and Euler's methods are more novel developments in pursuit of
accurately representing the spread of the virus \cite{APPADU2020}.

\section{Data Preprocessing}

Now moving on to actually wrangling the data for processing and straightforward
input to our prospective models, we first needed to examine the existing data
to analyze where help was needed. Upon inspection of data types, it is noted
that much of our data is numerical, bar the categorical and ordinal columns of
Province\_State and Date, respectively. We chose to convert the date from a
string type to datetime object type, to ease future parsing. Using our
contextual knowledge of our problem space, we chose to address the modeling
problem from an individual Province\_State perspective, i.e. we grouped our
data by the Province\_State field. From here on, we will refer to this
“Province\_State” field simply as the “state”. As a group we went back and
forth on whether having so many models would result in higher variance due to
the increased complexity of the model, but we decided on separate models due to
different restrictions imposed by policy makers on interstate travel and the
general thought that most people would be sheltering in place, and thus would
not travel between states.

\subsection{Round 1}

Our initial imputation strategy went as follows: We examined each state’s
numeric data column by column. If null values comprised more than 70\% of the
total data in the column, we dropped the entire column; otherwise, we replaced
all null values with the median value in our column. After noting that this
approach led to “plateaus” in our data, we tried linearly interpolating the
values in each column. This approach produces more organic data in each column,
but fails to impute data at the beginning of a time series (due to having no
“base point” to interpolate off of). Because our current models treat every
column separately, we don’t anticipate that this will cause issues; we can drop
NaN values to create new “columns” that are perfectly interpolated, but start
at a later date.  

After interpolating the data, we normalized it to zero mean unit variance. At
this time, we are still considering whether to normalize the data globally or
within a single column. Intuition would suggest that a column-wise approach
would be best. We performed these steps before really definitively knowing what
sort of modeling approach we were interested in taking, as we will discuss,
this normalization was ultimately unused, since we were working with more
direct regression methods that did not benefit from this scaling, but did drop
the null data points.

\subsection{Round 2}

At this point in time, we decided on scrapping our explorations of different
forecasting implementations to be detailed below, and focused on univariate
feature fitting methodologies for forecasting a function to fit the observed
count of confirmed cases and deaths, namely that of Holt-Winters exponential
smoothing, ARIMA for forecasting, and a simple linear model from a particular
day offset. As a result, we did not normalize our data and just allowed the
models to drop the null values from the training dataset.


\section{Problem Definition}

%% TODO: might be a bit too long and too detailed 
%% not sure what to edit so I just wrote a lot (easier to cut than to write more)

Currently with surging cases and deaths, the medical infrastructure in the
United States cannot keep up. They cannot prepare for these pandemic surges and
thus would need to find a way to predict when they will happen. Thus, the
problem set to be solved in these experiments is to create a method utilizing
the foundations and skills learned from CS 145 at UCLA to predict the cases in
each state of the United States for a given amount of time. 

%%TODO: cite the dataset

The competition is divided into 2 rounds. The first round was to predict the
case and death count of each of the 50 states from September 1st to September
26th given any COVID-19 related data from April 12th to August 31st. The second
round was to predict the case and death count of each of the 50 states during
the winter holiday surge, specifically from December 7th to December 12th. For
the second round, the method can train on any any COVID-19 related data from
April 12th to December 6th. 

The accuracy of the prediction will be evaluated with the mean absolute
percentage error (MAPE) which averages the absolute error on all data points.
Equation is given below.

$$MAPE = \frac{1}{n} \sum_{i = 1}^{n} |\frac{p_i - a_i}{a_i} |$$

$n$ symbolizes the size of the dataset. $p_i$ symbolizes the predicted value at
index $i$. $a_i$ symbolizes the actual value at index $i$.


\section{Model Performance}
\subsection{Linear Regression}
\subsection{ARIMA}
Given the issue of producing predictions of confirmed cases and deaths without
having access to the values of the other features we would want to include in 
our model for the predicting time period. It was clear that time series algorithms
would be the best class of models for our data. The first model tried was the
Autoregressive (AR) model. The AR model uses a linear combination of previous
time steps to predict the future time steps. The equation for this model is \cite{forecasting},
\begin{equation}
 y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t 
\end{equation}
Where $y_t$ denotes the number of cases at time $t$, $\phi_j$ are the learned weights,
 $\epsilon_t$ is white noise and $c$ is a bias term. In this case we are looking at the last $p$ time days
for prediction, we denote this as an AR(p) model. 

To implement the model we use the 'AutoReg' module from the statsmodels package \cite{statsmodels}
and trained a unique AR model for each state. Since this model
was mainly explored during round 1, we used the data from the 2nd of August 2020 to the 31st of August 2020
as validation data. From this validation data we tuned $p$ to get the lowest overall mape for confirmed cases and deaths.
This led to an optimal $p$ of 3, however, our MAPE was still very high at $11.3$. What we saw in the model was that our
confirmed cases diverged significantly from the true values as time went on. When the AR model predicts future unseen values,
we have shown that the model must use previous time steps to predict the current one. Hence when looking at unseen data, 
the model will use previously predicted values to predict future values. This leads to a compounding of error and thus 
results in a diverging relationship between the true values and the predicted ones as can be seen in figure 1.
\begin{figure}
  \includegraphics[width=\linewidth]{./figures/Section2_AR_Cali_Conf.png}
  \caption{Confirmed Cases in California predicted using AR}
\end{figure} 
Our final round 1 MAPE using AR was $4.12$
The issue with our implementation of the model is that we did not enforce the stationarity of the data which 
is a prerequisite for the model. A stationary time series is one whose properties do not depend on the time at 
which the series is observed \cite{forecasting}.
This therefore shows AR's difficulty in accounting for the trend since stationary
data is devoid of one. Therefore we needed a model that was able to account for data with a trend. This is why we chose to 
explore the Autoregressive Integrated Moving Average (ARIMA) model. The Moving Average (MA) portion of ARIMA states that 
future time steps are predicted as a linear combination of previous forecast errors. The equation for this model is \cite{forecasting},
\begin{equation}
y_t = c + \sum_{i=1}^{q} \theta_i \epsilon_{t-i}
\end{equation} 
Where $\theta_i$ are the learned weights, $c$ is the bias term and $\epsilon_i$ are the white noise error terms that are 
seen in equation 1. Since we look at the last $q$ time steps, this is known as a MA(q) model.
Hence, a combination of the AR model and the MA model will reduce the compounding error problem from before since we
take the forecast errors into account. However both AR and MA models require data to be stationary which as shown before is not
the case. A way to make data stationary is by looking at the difference of contiguous time series values $y'_t = y_t - y_{t-1}$
as opposed to the values themselves. This process of differencing can be done multiple times if the initial differencing does
not yield a stationary time series.

Therefore, the ARIMA model incorporates differencing to give the forecasting equation \cite{forecasting},
\begin{equation}
  y'_t = c + \sum_{i = 1}^{p} \phi_i y'_{t-i} + \sum_{j=1}^{q} \theta_j \epsilon_{t-j} + \epsilon_t
\end{equation}
Which we see is a combination of equations 1 and 2 but on differenced values. Thus the $\epsilon_j$ are white noise errors for
$y'_j$ not the original time series $y_j$. As we mentioned, differencing can occur many times until stationarity is enforced,
we let the the number of time the data is differenced be denoted as $d$ the degree of differencing. Furthermore, we are
already aware of the parameters $p$ and $q$ from the respective AR and MA models. Hence we denote the above equation as an
ARIMA(p,d,q) model. 

To implement this model, we used the ARIMA module from the statsmodels package \cite{statsmodels}. There are, however, many
ways to find the optimal $p,d,q$ values. Initially we thought that since our metric of evaluation is MAPE, we could optimize
these parameters with respect to a validation MAPE and use those parameters to predict the future cases and deaths for a 
particular state. To do this, a simple grid search was done where $p,d,q$ values ranged from $1$ to $10$ and the tuple with the
lowest validation MAPE for that state was chosen. Initially we chose a validation size of 30 days. This yielded a final round 1 MAPE of $3.44$.
Although this was better than the AR model, it was still below the baseline MAPE. In addition, we saw an increased number 
of stationarity and convergence errors with our optimized set of tuples for certain states. In these cases, we had to resort to
using our naive AR implementation as backup. This indicated that our tuple was not fully capturing the trends in our data
by being optimizing for 30 days prior. 

Hence we chose to reduce our validation size to 7 days instead to see if having a larger amount of data and using a more
recent validation set would lead to a more representative optimal $p,d,q$ tuple. This was indeed the case as we were able to
get a round 1 MAPE of 2.96. We still ran into the stationarity and convergence errors and did have to use AR as a backup.
In this vein, we reduced the validaiton size further to only 3 days but this led to a MAPE of 4.71 indicating that we overfit
our validation data. 

The auto arima module from the pmdarima package \cite{pmdarima} uses a grid search similar to before to find the optimal
tuple. However, auto arima uses statistically rigorous stationarity tests (such as Augmented Dickey Fuller) to find 
the optimal order of differencing $d$ after which the optimal $p,q$ are found by minimizing the Akaike Information Criterion (AIC).
AIC is a measure of the quality of a model, it measures the likelihood of a model to predict future values \cite{AIC}. The equation
to calculate AIC is given by \cite{AIC},
\begin{equation}
  AIC  = -2 \cdot \ln(L) + 2k
\end{equation} 
Where $L$ is the likelihood of the model, and $k$ is the number of estimated parameters. A model with a lower AIC value
indicates a model of better quality. Optimizing on by MAPE on a validation set runs the risk of being overfit to the validation
set as seen when we reduced our validation size to 3 days. Hence when looking a generalizable models for our round 2 submission,
it was important that we did not overfit to our current data. By using a measure that is trained insample and does not depend
on a validation set, the generalizability of the model increases. By optimzing for AIC we did get a lower MAPE than our previous
models so this was chosen as the optimization criteria when a state is trained using this method.


\subsection{HOLT}
\subsection{Exponential Smoothing}
\subsection{Long Short Term Memory Neural Networks}

Neural networks are shown to be incredibly malleable since there is a plentiful
amount of hyperparameters to tune and to create ranging from the type of
activation function to the architecture of the network. For sequential data
specifically, recurrent neural networks or RNNs work well mostly because it
allows previous outputs to be used as inputs while having hidden states. 

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/architecture-rnn-ltr.png}
  \caption{Example of RNN: $x$ is the input, $y$ is the output, the middle blue
  cells are a part of the hidden layer}
  \label{fig:rnn}
\end{figure}

In the context of time series prediction and the task at hand, RNNs work well
since it can utilize previous timestamped or predicted data to predict other
output data. Inside each layer and node are specific functions that manipulate
the inputs known as gates.

Within the context of RNNs, there is a set of specific RNNs called Long Short
Term Memory Neural Networks or LSTM for short. What is specific of the LSTM is
that it utilizes a specific set of 3 gates: an update, output, relvance, and
forget gate \cite{LSTMlecture}. This allows LSTM neural networks to regulate
the flow of information and decide how much past data affects the current
prediction, generalizing better for time-series specific tasks.

Given this context, and before using this method as our main modeling choice,
we needed to validate this method. Using the PyTorch neural network framework
\cite{Pytorch}, a simple RNN neural network of 1 layer with 50 LSTM nodes was
tested on a subset of the round 1 data. 

The experimentation procedure would use the confirmed case and death count data
from April 12th to August 17th for the state of California and then validating
the MAPE over a period of 14 days, August 18th to August 31st. The neural
network will be trained over a varied amount of epochs and learning rates. 

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/LSTMPytorch.png}
  \caption{LSTM predictions on data pertaining to California}
  \label{fig:LSTMtrial}
\end{figure}

Attached on Figure  \ref{fig:LSTMtrial} is an example trial run of our
prediction using the PyTorch neural network. The MAPE on this was around $6.93$
which was much higher than the other methods.

Given the lackluster results, amount of time required to tune each parameter,
and the non-deterministic nature of neural networks, the use of LSTM neural
networks was not really explored deeply with the rest of the dataset.

\subsection{Conclusions}

While the HOLT model typically gives us the lowest overall MAPE, it
consistently underperforms on certain states. Linear Regression and ARIMA may
yield significantly better results for these "problem states". However, it is
not always easy to tell which states are going to cause problems with the HOLT
model -- performance varies significantly based on the shape of the latest
batch of test data. This heterogeneity makes it very difficult to pick a
"favored model" for our time series predictions. 

Not all of our results were mixed. While model performance varied significantly
based on the target state, the "optimal hyperparameters" used to train these
models do not change significantly on a state-by-state basis. By performing
grid search on each model's hyperparameters, we developed a group of
parameterized models that handle general state data reasonably well. 


\section{Final Model Design}

The pros and cons (as well as the relative efficacy) of each model have already
been discussed. Faced with such a diversity of model performance, we opted to
use a hybrid model for our overall submission. 

Both rounds used a univariate data model (ie. trained only on the number of
deaths or confirmed cases), and were trained state-by-state (ie. a new model
was trained on each state's data; this model's predictions became our
predictions for the given state). An overview of our methodology follows. 

\subsection{Round 1}

Our Round 1 modelling method used a comparatively straightfoward "hybrid"
approach. We split our training data by state, then manually ran HOLT and
linear regression models on the state's data (using the last few weeks as
validation data). We noted which models gave us a lower MAPE for which states,
and hardcoded these $(state, model)$ associations into our script, assuming
that these trends would hold. For example, linear regression usually
outperformed HOLT for predicting deaths in the state of Montana; we accordingly
instructed the computer to always use linear regression when predicting
Montana's deaths. 

However, this mapping wasn't sufficient. 
Model performance is a function of the total amount of data given to the model;
for time series data, this includes our start date. Linear regression, for
example, may perform better when given only the last 2 weeks of data (to
understand why, consider the effect of step size on linear gradient descent
when trying to fit an exponential curve). 
Every model was thus given a \emph{start date} parameter, which was defined as
the date of the oldest training datapoint that the model received. 
For Round 1, we set this parameter manually on a state-by-state basis by
hardcoding it into our script. 

After building up this database of optimal start dates and optimal models, we
wrote a Python script that trained the given models (using \cite{scikit-learn}'s
\texttt{linear\_model.LinearRegression} class and the \texttt{Holt} module from
the \cite{statsmodels} library). We then used the library routines to give us
time series predictions for the required number of days. 

\subsection{Round 2}

After completing Round 1 of the project, we began to realize that manually
cherry-picking an appropriate model for each state was not scalable. 

We had no way to test quick adjustments to our hyperparameters, which made
manual optimization tedious. In addition, we had no way of effectively
determining the best \emph{start\_date} for a given model on a given state. 
We opted to shift towards an automated approach towards model selection. 
Otherwise, our methodology remained the same as in Round 1. 

Suppose that we are given $n$ days of training data, and are tasked with
predicting $m$ days in the future. 

First, we segregate the data again. Given $n$ days of training data, our model
uses the first $n-2$ days to train on (we'll call this set $T$) and the last
$2$ days as a validation dataset (we'll call this subdivision $V$). We then
train Linear Regression, HOLT, and ARIMA models on $T$, predicting $m+2$ days
into the future. We take the first $2$ days of these predictions, and calculate
the MAPE using $V$. After comparing the MAPEs of each model on $V$, we choose
the model that gives us the lowest MAPE for that particular state, and return
the remaining $m$ predicted days. 

After observing our hybrid model, we noted two trends: 
\begin{itemize}
\item 
Splitting our data into $T$ and $V$ caused some of our models to
underperform. This is because they effectively had to predict $m+2$ days into
the future as opposed to $m$ days (the first two days, of course, were used for
our validation and model selection). This underperformance wasn't uniform;
HOLT tended to give us relatively accurate predictions, while linear regression
tended to underpredict the number of deaths. 
\item
Different models often give similar MAPEs when evaluated on $V$. 
\end{itemize}

Given these two points, we added an $\epsilon$ "margin" term to our final
implementation. If two models perform comparatively (ie. within $\epsilon$ MAPE
of each other) on $V$, then our framework prefers the model that "generalizes
better" as per our heuristic observations. For example, if linear regression
and HOLT both get an MAPE of $8.675309 \pm (\epsilon = 0.001)$, our framework
will use HOLT to predict the values for the state. Our framework prefers HOLT
to linear regression if the MAPEs are similar, but doesn't put ARIMA into this
"margin" framework (ie. if ARIMA yields the lowest MAPE, ARIMA will always be
chosen). The addition of this $\epsilon$ parameter yielded an experimental 
MAPE improvement of around 0.05. 

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}
\endinput
