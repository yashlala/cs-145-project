\documentclass[sigconf,nonacm]{acmart}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

\title{CS145 Team 22 Midterm Report}

\author{Hamlin Liu}
\affiliation{%
  \institution{UCLA, 805103522}
  }
\email{hamlin.liu@gmail.com}
\author{Shriniket Buche}
\affiliation{%
  \institution{UCLA, 305088562}
  }
\email{shriniketbuche@gmail.com}
\author{Juan Estrada}
\affiliation{%
  \institution{UCLA, 105347991}
  }
\email{juanestrada@ucla.edu}
\author{Yash Lala}
\affiliation{%
  \institution{UCLA, 905159212}
  }
\email{yashlala@gmail.com}
\author{Justin Yi}
\affiliation{%
  \institution{UCLA, 905123893}
  }
\email{joostinyi00@gmail.com}
\renewcommand{\shortauthors}{Team 22}

\begin{abstract}

COVID-19, the theme of our CS 145 project, has been classified as a worldwide pandemic. 
The rate of its spread is a complex function of local social distancing, government policy, 
climate, and temporal factors, just to name a few. Detailed analysis of all of these contributory 
causes is not humanly possible. However, we can build reasonably accurate disease models by using the 
Data Mining techniques taught in CS 145. This report documents our team’s efforts towards analyzing 
COVID-related data in over the majority of the year to allow for predictive forecasting. 

We catalogue our problem at hand, initial data preprocessing, exploratory efforts toward modeling our problem, 
and our future predictions as we construct a model capable of predicting fatalities caused by COVID-19, 
namely through the use of a hybrid linear, exponential, and autoregressive models. 

\end{abstract}

\maketitle

\section{Introduction of overall goal and background}
The increasing spread of COVID-19 poses a substantial impact on the status of global health and economy, 
and having accurate forecasts of the number of affected can better inform local and national governmental 
bodies to make decisions to promote public health and safety and keeping the economy afloat. 
As we position to tackle this problem, we inspect our the data we have on hand: we were given reports of various 
state-wide features of the COVID-19 infection profile from as early as April, we are specifically interested in the ability 
to predict the number of cases and deaths for a particular state.

\section{Related Work}
Notable advances in this domain are noted, Nesteruk \cite{Nesteruk} employs the Susceptible, Infected, Removed (SIR) paradigm
– which aims to find different correlated differential equations to model flow between different partitions of the 
studied population – and the subsequent progression of the pandemic. Yonar \cite{EJMO} uses different more direct curve estimation methods
on the number of coronavirus cases, such as Holt and ARIMA, for different affected countries of interest, (e.g. Germany, Japan, Turkey, UK, .etc),
which are shown to be statistically significant. Different iterative methods, based on cubic spline interpolation and Euler's methods
are more novel developments in pursuit of accurately representing the spread of the virus \cite{APPADU2020}.

\section{Data Preprocessing}
Now moving on to actually wrangling the data for processing and straightforward input to our prospective models, 
we first needed to examine the existing data to analyze where help was needed. Upon inspection of data types, it is noted 
that much of our data is numerical, bar the categorical and ordinal columns of Province\_State and Date, respectively. 
We chose to convert the date from a string type to datetime object type, to ease future parsing. 
Using our contextual knowledge of our problem space, we chose to address the modeling problem from an individual 
Province\_State perspective, i.e. we grouped our data by the Province\_State field. 
From here on, we will refer to this “Province\_State” field simply as the “state”. 
As a group we went back and forth on whether having so many models would result in higher variance due to the increased complexity of the model, 
but we decided on separate models due to different restrictions imposed by policy makers on interstate travel and 
the general thought that most people would be sheltering in place, and thus would not travel between states.

\subsection{Round 1}
Our initial imputation strategy went as follows: 
We examined each state’s numeric data column by column. If null values comprised more than 70\% of the total data in the column, 
we dropped the entire column; otherwise, we replaced all null values with the median value in our column. 
After noting that this approach led to “plateaus” in our data, we tried linearly interpolating the values in each column. 
This approach produces more organic data in each column, but fails to impute data at the beginning of a time series 
(due to having no “base point” to interpolate off of). Because our current models treat every column separately, 
we don’t anticipate that this will cause issues; we can drop NaN values to create new “columns” that are perfectly interpolated, but start at a later date.  
\\
After interpolating the data, we normalized it to zero mean unit variance. 
At this time, we are still considering whether to normalize the data globally or within a single column. 
Intuition would suggest that a column-wise approach would be best. We performed these steps before really definitively knowing 
what sort of modeling approach we were interested in taking, as we will discuss, this normalization was ultimately unused, 
since we were working with more direct regression methods that did not benefit from this scaling, but did drop the null data points.

\subsection{Round 2}
At this point in time, we decided on scrapping our explorations of different forecasting implementations to be detailed below, 
and focused on univariate feature fitting methodologies for forecasting a function to fit the observed count of confirmed cases and deaths, 
namely that of Holt-Winters exponential smoothing, ARIMA for forecasting, and a simple linear model from a particular day offset. 
As a result, we did not normalize our data and just allowed the models to drop the null values from the training dataset.

\section{Model Performance}
\subsection{Linear Regression}
\subsection{ARIMA}
\subsection{HOLT}
\subsection{Conclusions}

While the HOLT model typically gives us the lowest overall MAPE, it
consistently underperforms on certain states. Linear Regression and ARIMA may
yield significantly better results for these "problem states". However, it is
not always easy to tell which states are going to cause problems with the HOLT
model -- performance varies significantly based on the shape of the latest
batch of test data. This heterogeneity makes it very difficult to pick a
"favored model" for our time series predictions. 

Not all of our results were mixed. While model performance varied significantly
based on the target state, the "optimal hyperparameters" used to train these
models do not change significantly on a state-by-state basis. By performing
grid search on each model's hyperparameters, we developed a group of
parameterized models that handle general state data reasonably well. 


\section{Predictor Design}

The pros and cons (as well as the relative efficacy) of each model have already
been discussed. Faced with such a diversity of model performance, we opted to
use a hybrid model for our overall submission. 

Both rounds used a univariate data model (ie. trained only on the number of
deaths or confirmed cases), and were trained state-by-state (ie. a new model
was trained on each state's data; this model's predictions became our
predictions for the given state). An overview of our methodology follows. 

\subsection{Round 1}

Our Round 1 modelling method used a comparatively straightfoward "hybrid"
approach. We split our training data by state, then manually ran HOLT and
linear regression models on the state's data (using the last few weeks as
validation data). We noted which models gave us a lower MAPE for which states,
and hardcoded these $(state, model)$ associations into our script, assuming
that these trends would hold. For example, linear regression usually
outperformed HOLT for predicting deaths in the state of Montana; we accordingly
instructed the computer to always use linear regression when predicting
Montana's deaths. 

However, this mapping wasn't sufficient. 
Model performance is a function of the total amount of data given to the model;
for time series data, this includes our start date. Linear regression, for
example, may perform better when given only the last 2 weeks of data (to
understand why, consider the effect of step size on linear gradient descent
when trying to fit an exponential curve). 
Every model was thus given a \emph{start date} parameter, which was defined as
the date of the oldest training datapoint that the model received. 
For Round 1, we set this parameter manually on a state-by-state basis by
hardcoding it into our script. 

After building up this database of optimal start dates and optimal models, we
wrote a Python script that trained the given models (using \cite{scikit-learn}'s
\texttt{linear_model.LinearRegression} class and the \texttt{Holt} module from
the \cite{statsmodels} library). We then used the library routines to give us
time series predictions for the required number of days. 


\subsection{Round 2}


After completing Round 1 of the project, we began to realize that manually
cherry-picking an appropriate model for each state was not scalable. 

We had no way to test quick adjustments to our hyperparameters, which made
manual optimization tedious. In addition, we had no way of effectively
determining the best \emph{start_date} for a given model on a given state. 
We opted to shift towards an automated approach towards model selection. 
Otherwise, our methodology remained the same as in Round 1. 

Suppose that we are given $n$ days of training data, and are tasked with
predicting $m$ days in the future. 

First, we segregate the data again. Given $n$ days of training data, our model
uses the first $n-2$ days to train on (we'll call this set $T$) and the last
$2$ days as a validation dataset (we'll call this subdivision $V$). We then
train Linear Regression, HOLT, and ARIMA models on $T$, predicting $m+2$ days
into the future. We take the first $2$ days of these predictions, and calculate
the MAPE using $V$. After comparing the MAPEs of each model on $V$, we choose
the model that gives us the lowest MAPE for that particular state, and return
the remaining $m$ predicted days. 

After observing our hybrid model, we noted two trends: 
\begin{itemize}
\item 
Splitting our data into $T$ and $V$ caused some of our models to
underperform. This is because they effectively had to predict $m+2$ days into
the future as opposed to $m$ days (the first two days, of course, were used for
our validation and model selection). This underperformance wasn't uniform;
HOLT tended to give us relatively accurate predictions, while linear regression
tended to underpredict the number of deaths. 
\item
Different models often give similar MAPEs when evaluated on $V$. 
\end{itemize}

Given these two points, we added an $\epsilon$ "margin" term to our final
implementation. If two models perform comparatively (ie. within $\epsilon$ MAPE
of each other) on $V$, then our framework prefers the model that "generalizes
better" as per our heuristic observations. For example, if linear regression
and HOLT both get an MAPE of $8.675309 \pm (\epsilon = 0.001)$, our framework
will use HOLT to predict the values for the state. Our framework prefers HOLT
to linear regression if the MAPEs are similar, but doesn't put ARIMA into this
"margin" framework (ie. if ARIMA yields the lowest MAPE, ARIMA will always be
chosen). The addition of this $\epsilon$ parameter yielded an experimental 
MAPE improvement of around 0.05. 

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}
\endinput
